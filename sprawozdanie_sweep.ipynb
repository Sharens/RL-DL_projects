{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273096db",
   "metadata": {},
   "source": [
    "# Sprawozdanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2165bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Biblioteki i funkcje pomocnicze załadowane\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SEKCJA 1: Import bibliotek i funkcje pomocnicze\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funkcja: Rozwiązanie równania Bellmana dla polityki\n",
    "def evaluate_policy_linear_system(P_pi: np.ndarray, r_pi: np.ndarray, gamma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rozwiązuje równanie Bellmana dla danej polityki w postaci macierzowej.\n",
    "    \n",
    "    Dla polityki π mamy:\n",
    "        v = r_pi + gamma * P_pi * v\n",
    "    czyli:\n",
    "        (I - gamma * P_pi) v = r_pi\n",
    "    \n",
    "    Parametry:\n",
    "    -----------\n",
    "    P_pi : ndarray (nS x nS)\n",
    "        Macierz przejść dla polityki π\n",
    "    r_pi : ndarray (nS,)\n",
    "        Wektor nagród oczekiwanych dla polityki π\n",
    "    gamma : float\n",
    "        Współczynnik dyskontowania\n",
    "    \n",
    "    Zwraca:\n",
    "    -------\n",
    "    v : ndarray (nS,)\n",
    "        Funkcja wartości dla polityki π\n",
    "    \"\"\"\n",
    "    nS = P_pi.shape[0]\n",
    "    I = np.eye(nS)\n",
    "    return np.linalg.solve(I - gamma * P_pi, r_pi)\n",
    "\n",
    "# Funkcja pomocnicza: Wyświetlenie wartości jako siatkę\n",
    "def pretty_matrix_as_grid(v: np.ndarray, nrow: int, ncol: int, decimals: int = 1):\n",
    "    \"\"\"Wyświetl wektor wartości jako siatkę (nrow x ncol).\"\"\"\n",
    "    grid = v.reshape(nrow, ncol)\n",
    "    with np.printoptions(precision=decimals, suppress=True):\n",
    "        print(grid)\n",
    "\n",
    "# Funkcja pomocnicza: Wizualizacja polityki jako strzałki\n",
    "def action_arrows(pi_det: np.ndarray, nrow: int, ncol: int):\n",
    "    \"\"\"Zamienia deterministyczną politykę na strzałki w siatce.\"\"\"\n",
    "    arrows = {0:'↑', 1:'→', 2:'↓', 3:'←', None:'·'}\n",
    "    out = []\n",
    "    for r in range(nrow):\n",
    "        row = []\n",
    "        for c in range(ncol):\n",
    "            s = r*ncol + c\n",
    "            a = int(pi_det[s]) if pi_det[s] is not None else None\n",
    "            row.append(arrows.get(a, '?'))\n",
    "        out.append(' '.join(row))\n",
    "    print('\\n'.join(out))\n",
    "\n",
    "print(\"✓ Biblioteki i funkcje pomocnicze załadowane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54022313",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: Komponenty MDP - Środowisko vs Agent\n",
    "\n",
    "### Trzy kluczowe komponenty MDP:\n",
    "\n",
    "#### 1. **Model Środowiska (Environment Model) - P[s][a]**\n",
    "- Definiuje dynamikę świata\n",
    "- Zawiera informacje o przejściach między stanami i nagrodach\n",
    "- Format: `P[s][a] → [(p, s', r, terminated), ...]`\n",
    "  - `p`: prawdopodobieństwo przejścia\n",
    "  - `s'`: następny stan\n",
    "  - `r`: nagroda\n",
    "  - `terminated`: czy epizod się kończy\n",
    "\n",
    "**Przykład**: Recycling Robot zdefiniowany parametrami:\n",
    "- `α` (alpha): prawdopodobieństwo pozostania w stanie H po SEARCH\n",
    "- `β` (beta): prawdopodobieństwo pozostania w stanie L po SEARCH\n",
    "- `r_search`, `r_wait`: nagrody za akcje\n",
    "- `rescue_cost`: kara za rozładowanie\n",
    "\n",
    "#### 2. **Polityka Agenta (Policy) - π[s,a]**\n",
    "- Definiuje zachowanie agenta\n",
    "- Macierz rozmiar: `(nStanów, nAkcji)`\n",
    "- `π[s,a]` = prawdopodobieństwo wybrania akcji `a` w stanie `s`\n",
    "- Dla polityki deterministycznej: `π[s,a] ∈ {0, 1}`\n",
    "\n",
    "**Przykład**: π(H)=SEARCH, π(L)=RECHARGE oznacza:\n",
    "- W stanie H zawsze szukaj (SEARCH)\n",
    "- W stanie L zawsze ładuj (RECHARGE)\n",
    "\n",
    "#### 3. **Funkcja Wartości (Value Function) - v_π[s]**\n",
    "- Mierzy oczekiwany zdyskontowany zwrot\n",
    "- `v_π(s) = E_π[∑_{t=0}^∞ γ^t R_{t+1} | S_t = s]`\n",
    "- Odpowiada na pytanie: \"Ile średnio zyskamy w przyszłości startując ze stanu s i stosując politykę π?\"\n",
    "\n",
    "### Relacja między komponentami:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────┐\n",
    "│  ŚRODOWISKO (Environment)           │\n",
    "│  Model P[s][a]                      │\n",
    "│  - przejścia między stanami          │\n",
    "│  - nagrody                           │\n",
    "│  (definiuje możliwości świata)       │\n",
    "└──────────────┬──────────────────────┘\n",
    "               │\n",
    "               ↓ (Agent w nim funkcjonuje)\n",
    "               \n",
    "┌──────────────────────────────────────┐\n",
    "│  AGENT                               │\n",
    "│  Polityka π[s,a]                     │\n",
    "│  - decyzje w każdym stanie           │\n",
    "│  (definiuje sposób działania agenta) │\n",
    "└──────────────┬──────────────────────┘\n",
    "               │\n",
    "               ↓ (Razem dają wynik)\n",
    "               \n",
    "┌──────────────────────────────────────┐\n",
    "│  Funkcja Wartości v_π[s]             │\n",
    "│  - oczekiwane zwroty                 │\n",
    "│  (ocena jakości polityki)            │\n",
    "└──────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Wniosek**: Parametry świata (α, β, rescue_cost, gamma) **NIE są polityką**. Ale wpływają na wartości akcji, co zmienia **która polityka jest optymalna**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fcfaa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Model Recycling Robot - Budowa P[s][a]\n",
    "\n",
    "### Opis problemu\n",
    "\n",
    "**Recycling Robot** to prosty MDP z 2 stanami reprezentującymi poziom energii baterii:\n",
    "- **Stan H** (High): wysoka energia\n",
    "- **Stan L** (Low): niska energia\n",
    "\n",
    "**Dostępne akcje:**\n",
    "1. **SEARCH (0)**: szuka puszek (nagroda +5), ale może rozładować baterię\n",
    "2. **WAIT (1)**: czeka na człowieka (nagroda +1), brak rozładowania\n",
    "3. **RECHARGE (2)**: ładuje baterię (tylko w stanie L, nagroda 0)\n",
    "\n",
    "### Dynamika przejść:\n",
    "\n",
    "| Stan | Akcja | Następny stan | Prawdopodobieństwo | Nagroda |\n",
    "|------|-------|----------------|-------------------|---------|\n",
    "| H | SEARCH | H | α | r_search |\n",
    "| H | SEARCH | L | 1-α | r_search |\n",
    "| H | WAIT | H | 1.0 | r_wait |\n",
    "| L | SEARCH | L | β | r_search |\n",
    "| L | SEARCH | H | 1-β | rescue_cost |\n",
    "| L | WAIT | L | 1.0 | r_wait |\n",
    "| L | RECHARGE | H | 1.0 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ace5494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST: Budowa modelu Recycling Robot\n",
      "======================================================================\n",
      "  ✓ H-SEARCH: suma prawdopodobieństw = 1.0000\n",
      "  ✓ H-WAIT: suma prawdopodobieństw = 1.0000\n",
      "  ✓ L-SEARCH: suma prawdopodobieństw = 1.0000\n",
      "  ✓ L-WAIT: suma prawdopodobieństw = 1.0000\n",
      "  ✓ L-RECHARGE: suma prawdopodobieństw = 1.0000\n",
      "  ✓ H-RECHARGE: akcja niedostępna (pusta lista)\n",
      "\n",
      "✓ Wszystkie testy przeszły pomyślnie!\n",
      "\n",
      "Model zawiera:\n",
      "  - Liczba stanów (nS): 2\n",
      "  - Liczba akcji (nA): 3\n",
      "  - Razem możliwych par (s,a): 6\n"
     ]
    }
   ],
   "source": [
    "def build_recycling_robot_P(alpha=0.8, beta=0.4, r_search=5.0, r_wait=1.0, rescue_cost=-3.0):\n",
    "    \"\"\"\n",
    "    Buduje model MDP dla Recycling Robot.\n",
    "    \n",
    "    Parametry:\n",
    "    ----------\n",
    "    alpha : float\n",
    "        Prawdopodobieństwo pozostania w H po SEARCH (domyślnie 0.8)\n",
    "    beta : float\n",
    "        Prawdopodobieństwo pozostania w L po SEARCH (domyślnie 0.4)\n",
    "    r_search : float\n",
    "        Nagroda za SEARCH (domyślnie 5.0)\n",
    "    r_wait : float\n",
    "        Nagroda za WAIT (domyślnie 1.0)\n",
    "    rescue_cost : float\n",
    "        Kara za rozładowanie w L (domyślnie -3.0)\n",
    "    \n",
    "    Zwraca:\n",
    "    -------\n",
    "    P : dict\n",
    "        Model świata P[s][a] -> [(p, s', r, terminated), ...]\n",
    "    nS : int\n",
    "        Liczba stanów (2)\n",
    "    nA : int\n",
    "        Liczba akcji (3)\n",
    "    \"\"\"\n",
    "    \n",
    "    nS, nA = 2, 3  # 2 stany, 3 akcje\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "    \n",
    "    # Aliasy dla czytelności\n",
    "    H, L = 0, 1\n",
    "    SEARCH, WAIT, RECHARGE = 0, 1, 2\n",
    "    \n",
    "    # ========== Stan H (wysoka energia) ==========\n",
    "    \n",
    "    # SEARCH w H: może pozostać w H (z p=alpha) lub przejść do L\n",
    "    P[H][SEARCH] = [\n",
    "        (alpha, H, r_search, False),\n",
    "        (1 - alpha, L, r_search, False),\n",
    "    ]\n",
    "    \n",
    "    # WAIT w H: zawsze pozostaje w H\n",
    "    P[H][WAIT] = [\n",
    "        (1.0, H, r_wait, False),\n",
    "    ]\n",
    "    \n",
    "    # RECHARGE w H: akcja niedostępna\n",
    "    P[H][RECHARGE] = []\n",
    "    \n",
    "    # ========== Stan L (niska energia) ==========\n",
    "    \n",
    "    # SEARCH w L: może pozostać w L (z p=beta) lub rozładować się i wrócić do H\n",
    "    P[L][SEARCH] = [\n",
    "        (beta, L, r_search, False),\n",
    "        (1 - beta, H, rescue_cost, False),\n",
    "    ]\n",
    "    \n",
    "    # WAIT w L: zawsze pozostaje w L\n",
    "    P[L][WAIT] = [\n",
    "        (1.0, L, r_wait, False),\n",
    "    ]\n",
    "    \n",
    "    # RECHARGE w L: zawsze przechodzi do H z nagrodą 0\n",
    "    P[L][RECHARGE] = [\n",
    "        (1.0, H, 0.0, False),\n",
    "    ]\n",
    "    \n",
    "    return P, nS, nA\n",
    "\n",
    "\n",
    "# Test: Weryfikacja poprawności modelu\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST: Budowa modelu Recycling Robot\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "P, nS, nA = build_recycling_robot_P()\n",
    "\n",
    "# Sprawdzenie rozmiarów\n",
    "assert nS == 2 and nA == 3, \"Zły rozmiar MDP\"\n",
    "\n",
    "# Sprawdzenie że prawdopodobieństwa sumują się do 1 dla każdej akcji\n",
    "test_cases = [\n",
    "    (0, 0, \"H-SEARCH\"),\n",
    "    (0, 1, \"H-WAIT\"),\n",
    "    (1, 0, \"L-SEARCH\"),\n",
    "    (1, 1, \"L-WAIT\"),\n",
    "    (1, 2, \"L-RECHARGE\"),\n",
    "]\n",
    "\n",
    "for s, a, name in test_cases:\n",
    "    if P[s][a]:  # jeśli akcja jest dostępna\n",
    "        prob_sum = sum(p for p, *_ in P[s][a])\n",
    "        assert abs(prob_sum - 1.0) < 1e-12, f\"Błąd w {name}: suma = {prob_sum}\"\n",
    "        print(f\"  ✓ {name}: suma prawdopodobieństw = {prob_sum:.4f}\")\n",
    "\n",
    "# Sprawdzenie że RECHARGE w H jest niedostępna\n",
    "assert P[0][2] == [], \"RECHARGE w H powinien być niedostępny\"\n",
    "print(f\"  ✓ H-RECHARGE: akcja niedostępna (pusta lista)\")\n",
    "\n",
    "print(\"\\n✓ Wszystkie testy przeszły pomyślnie!\")\n",
    "print(\"\\nModel zawiera:\")\n",
    "print(f\"  - Liczba stanów (nS): {nS}\")\n",
    "print(f\"  - Liczba akcji (nA): {nA}\")\n",
    "print(f\"  - Razem możliwych par (s,a): {nS * nA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639f47b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: Ewaluacja Polityki - A1.2\n",
    "\n",
    "### Cel:\n",
    "Dla zadanej polityki obliczyć funkcję wartości $v_\\pi$ rozwiązując równanie Bellmana.\n",
    "\n",
    "### Procedura:\n",
    "\n",
    "1. **Budowa macierzy przejść i wektora nagród dla polityki**\n",
    "   - Ze zwykłego modelu `P[s][a]` i polityki `π` konstruujemy `P_π` i `r_π`\n",
    "   - Te obiekty reprezentują świat \"widziany przez politykę\"\n",
    "   \n",
    "2. **Rozwiązanie układu równań liniowych**\n",
    "   - Równanie Bellmana: $v_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi$\n",
    "   - Przeformułowanie: $(I - \\gamma P_\\pi) v_\\pi = r_\\pi$\n",
    "   - Rozwiązanie: $v_\\pi = (I - \\gamma P_\\pi)^{-1} r_\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bad4b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "A1.2: EWALUACJA POLITYKI - Porównanie różnych strategii\n",
      "======================================================================\n",
      "\n",
      "1. POLITYKA π₁: Szukaj w H, Ładuj w L\n",
      "   Logika: Szukamy gdy mamy energię, ładujemy gdy kończy się\n",
      "   v_π₁(H) = 42.3729  (wartość w stanie wysokiej energii)\n",
      "   v_π₁(L) = 38.1356  (wartość w stanie niskiej energii)\n",
      "   Średnia: 40.2542\n",
      "\n",
      "2. POLITYKA π₂: Czekaj wszędzie\n",
      "   Logika: Minimalizujemy ryzyko - nigdy nie szukamy\n",
      "   v_π₂(H) = 10.0000\n",
      "   v_π₂(L) = 10.0000\n",
      "   Średnia: 10.0000\n",
      "\n",
      "3. POLITYKA π₃: Szukaj wszędzie\n",
      "   Logika: Maksymalizujemy zbieranie puszek - zawsze szukamy\n",
      "   v_π₃(H) = 39.4634\n",
      "   v_π₃(L) = 33.6098\n",
      "   Średnia: 36.5366\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "PORÓWNANIE POLITYK:\n",
      "----------------------------------------------------------------------\n",
      "                Polityka    v(H)    v(L) Średnia\n",
      "π₁: Search-H, Recharge-L 42.3729 38.1356 40.2542\n",
      "       π₂: Wait wszędzie 10.0000 10.0000 10.0000\n",
      "     π₃: Search wszędzie 39.4634 33.6098 36.5366\n",
      "\n",
      "✓ NAJLEPSZA polityka: π₁\n",
      "  Uzasadnienie: Maksymalna średnia wartość = 40.2542\n",
      "\n",
      "WNIOSEK:\n",
      "----------------------------------------------------------------------\n",
      "• π₁ balansuję między bezpieczeństwem (RECHARGE w L) a zyskiem (SEARCH w H)\n",
      "• π₂ jest konserwatyjna - traci potencjał z SEARCH\n",
      "• π₃ jest agresywna - ale może czasem się rozładować w L\n",
      "• Dla domyślnych parametrów π₁ osiąga najlepszy wynik!\n"
     ]
    }
   ],
   "source": [
    "def build_P_r_for_policy(P, pi):\n",
    "    \"\"\"\n",
    "    Buduje (P_pi, r_pi) dla zadanej polityki π.\n",
    "    \n",
    "    Parametry:\n",
    "    ----------\n",
    "    P : dict\n",
    "        Model świata P[s][a] -> [(p, s', r, terminated), ...]\n",
    "    pi : ndarray (nS, nA)\n",
    "        Polityka: π[s,a] = P(A_t = a | S_t = s)\n",
    "    \n",
    "    Zwraca:\n",
    "    -------\n",
    "    P_pi : ndarray (nS, nS)\n",
    "        Macierz przejść dla polityki π\n",
    "    r_pi : ndarray (nS,)\n",
    "        Wektor nagród oczekiwanych dla polityki π\n",
    "    \n",
    "    Wyjaśnienie:\n",
    "    -----------\n",
    "    Z ogólnego MDP (P[s][a]) robimy \"świat widziany przez politykę π\"\n",
    "    poprzez uśrednianie po akcjach wybranych przez politykę.\n",
    "    \"\"\"\n",
    "    nS = len(P)\n",
    "    nA = len(P[0])\n",
    "    \n",
    "    P_pi = np.zeros((nS, nS), dtype=float)\n",
    "    r_pi = np.zeros(nS, dtype=float)\n",
    "    \n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            w = float(pi[s, a])  # Waga akcji a wg polityki π\n",
    "            \n",
    "            if w == 0.0:\n",
    "                continue  # Polityka nigdy nie wybiera tej akcji\n",
    "            \n",
    "            outcomes = P[s][a]\n",
    "            if not outcomes:\n",
    "                continue  # Akcja niedostępna\n",
    "            \n",
    "            # Sumujemy po wszystkich możliwych skutkach akcji\n",
    "            for (p, s2, r, terminated) in outcomes:\n",
    "                # Nagroda oczekiwana\n",
    "                r_pi[s] += w * p * float(r)\n",
    "                \n",
    "                # Przejścia (pomijamy stany terminalne)\n",
    "                if not terminated:\n",
    "                    P_pi[s, int(s2)] += w * p\n",
    "    \n",
    "    return P_pi, r_pi\n",
    "\n",
    "\n",
    "# ========== A1.2: Ewaluacja kilku polityk ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"A1.2: EWALUACJA POLITYKI - Porównanie różnych strategii\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gamma = 0.9  # Współczynnik dyskontowania\n",
    "P, nS, nA = build_recycling_robot_P()\n",
    "\n",
    "H, L = 0, 1\n",
    "SEARCH, WAIT, RECHARGE = 0, 1, 2\n",
    "\n",
    "# ===== POLITYKA 1: Search w H, Recharge w L =====\n",
    "print(\"\\n1. POLITYKA π₁: Szukaj w H, Ładuj w L\")\n",
    "print(\"   Logika: Szukamy gdy mamy energię, ładujemy gdy kończy się\")\n",
    "\n",
    "pi1 = np.zeros((nS, nA))\n",
    "pi1[H, SEARCH] = 1.0\n",
    "pi1[L, RECHARGE] = 1.0\n",
    "\n",
    "P_pi1, r_pi1 = build_P_r_for_policy(P, pi1)\n",
    "v1 = evaluate_policy_linear_system(P_pi1, r_pi1, gamma)\n",
    "\n",
    "print(f\"   v_π₁(H) = {v1[0]:.4f}  (wartość w stanie wysokiej energii)\")\n",
    "print(f\"   v_π₁(L) = {v1[1]:.4f}  (wartość w stanie niskiej energii)\")\n",
    "print(f\"   Średnia: {np.mean(v1):.4f}\")\n",
    "\n",
    "# ===== POLITYKA 2: Wait wszędzie =====\n",
    "print(\"\\n2. POLITYKA π₂: Czekaj wszędzie\")\n",
    "print(\"   Logika: Minimalizujemy ryzyko - nigdy nie szukamy\")\n",
    "\n",
    "pi2 = np.zeros((nS, nA))\n",
    "pi2[H, WAIT] = 1.0\n",
    "pi2[L, WAIT] = 1.0\n",
    "\n",
    "P_pi2, r_pi2 = build_P_r_for_policy(P, pi2)\n",
    "v2 = evaluate_policy_linear_system(P_pi2, r_pi2, gamma)\n",
    "\n",
    "print(f\"   v_π₂(H) = {v2[0]:.4f}\")\n",
    "print(f\"   v_π₂(L) = {v2[1]:.4f}\")\n",
    "print(f\"   Średnia: {np.mean(v2):.4f}\")\n",
    "\n",
    "# ===== POLITYKA 3: Search wszędzie =====\n",
    "print(\"\\n3. POLITYKA π₃: Szukaj wszędzie\")\n",
    "print(\"   Logika: Maksymalizujemy zbieranie puszek - zawsze szukamy\")\n",
    "\n",
    "pi3 = np.zeros((nS, nA))\n",
    "pi3[H, SEARCH] = 1.0\n",
    "pi3[L, SEARCH] = 1.0  # W L może dojść do rozładowania\n",
    "\n",
    "P_pi3, r_pi3 = build_P_r_for_policy(P, pi3)\n",
    "v3 = evaluate_policy_linear_system(P_pi3, r_pi3, gamma)\n",
    "\n",
    "print(f\"   v_π₃(H) = {v3[0]:.4f}\")\n",
    "print(f\"   v_π₃(L) = {v3[1]:.4f}\")\n",
    "print(f\"   Średnia: {np.mean(v3):.4f}\")\n",
    "\n",
    "# ===== PORÓWNANIE =====\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"PORÓWNANIE POLITYK:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "data = {\n",
    "    'Polityka': ['π₁: Search-H, Recharge-L', 'π₂: Wait wszędzie', 'π₃: Search wszędzie'],\n",
    "    'v(H)': [f\"{v1[0]:.4f}\", f\"{v2[0]:.4f}\", f\"{v3[0]:.4f}\"],\n",
    "    'v(L)': [f\"{v1[1]:.4f}\", f\"{v2[1]:.4f}\", f\"{v3[1]:.4f}\"],\n",
    "    'Średnia': [f\"{np.mean(v1):.4f}\", f\"{np.mean(v2):.4f}\", f\"{np.mean(v3):.4f}\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Znalezienie najlepszej polityki\n",
    "best_idx = np.argmax([np.mean(v1), np.mean(v2), np.mean(v3)])\n",
    "policies_names = ['π₁', 'π₂', 'π₃']\n",
    "print(f\"\\n✓ NAJLEPSZA polityka: {policies_names[best_idx]}\")\n",
    "print(f\"  Uzasadnienie: Maksymalna średnia wartość = {[np.mean(v1), np.mean(v2), np.mean(v3)][best_idx]:.4f}\")\n",
    "\n",
    "print(\"\\nWNIOSEK:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"• π₁ balansuję między bezpieczeństwem (RECHARGE w L) a zyskiem (SEARCH w H)\")\n",
    "print(\"• π₂ jest konserwatyjna - traci potencjał z SEARCH\")\n",
    "print(\"• π₃ jest agresywna - ale może czasem się rozładować w L\")\n",
    "print(\"• Dla domyślnych parametrów π₁ osiąga najlepszy wynik!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776fb0e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 5: Optymalizacja - Znajdowanie π* (A1.3)\n",
    "\n",
    "### Cel:\n",
    "Znaleźć **optymalną politykę** $\\pi_*$ sprawdzając wszystkie możliwe deterministyczne strategie.\n",
    "\n",
    "### Dlaczego brute force?\n",
    "- Robot ma tylko 2 stany\n",
    "- W każdym stanie max 3 akcje  \n",
    "- Liczba wszystkich polityk: 2 × 3 = **6 możliwości**\n",
    "- Możemy sprawdzić wszystkie i wybrać najlepszą!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcdb4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "A1.3: OPTYMALIZACJA - Znajdowanie najlepszej polityki π*\n",
      "======================================================================\n",
      "\n",
      "Optymalna polityka π* dla domyślnych parametrów:\n",
      "  Parametry: α=0.8, β=0.4, r_search=5.0, r_wait=1.0, rescue_cost=-3.0, γ=0.9\n",
      "\n",
      "  π*(H) = SEARCH\n",
      "  π*(L) = RECHARGE\n",
      "\n",
      "  Wartość: v_π*(H) = 42.3729\n",
      "  Wartość: v_π*(L) = 38.1356\n",
      "\n",
      "WNIOSEK:\n",
      "----------------------------------------------------------------------\n",
      "• Optymalnie jest szukać (SEARCH) gdy mamy energię (stan H)\n",
      "• Optymalnie jest ładować (RECHARGE) gdy energia się kończy (stan L)\n",
      "• Ta polityka osiąga najlepszy kompromis między zyskiem a bezpieczeństwem\n"
     ]
    }
   ],
   "source": [
    "def all_deterministic_policies_robot():\n",
    "    \"\"\"\n",
    "    Generuje wszystkie możliwe deterministyczne polityki dla robota.\n",
    "    \n",
    "    Zwraca:\n",
    "    -------\n",
    "    policies : list\n",
    "        Lista wszystkich 6 polityk, każda jako macierz (2, 3)\n",
    "    \"\"\"\n",
    "    H, L = 0, 1\n",
    "    SEARCH, WAIT, RECHARGE = 0, 1, 2\n",
    "    \n",
    "    policies = []\n",
    "    \n",
    "    # Dla każdej kombinacji akcji w H i L\n",
    "    for aH in [SEARCH, WAIT]:\n",
    "        for aL in [SEARCH, WAIT, RECHARGE]:\n",
    "            pi = np.zeros((2, 3))\n",
    "            pi[H, aH] = 1.0\n",
    "            pi[L, aL] = 1.0\n",
    "            policies.append(pi)\n",
    "    \n",
    "    return policies\n",
    "\n",
    "\n",
    "def best_policy_robot(P, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Znajduje optymalną politykę poprzez brute force.\n",
    "    \n",
    "    Parametry:\n",
    "    ----------\n",
    "    P : dict\n",
    "        Model MDP\n",
    "    gamma : float\n",
    "        Współczynnik dyskontowania\n",
    "    \n",
    "    Zwraca:\n",
    "    -------\n",
    "    best_pi : ndarray\n",
    "        Optymalna polityka\n",
    "    best_vH : float\n",
    "        Wartość w stanie H dla optymalnej polityki\n",
    "    \"\"\"\n",
    "    best_pi = None\n",
    "    best_vH = None\n",
    "    \n",
    "    for pi in all_deterministic_policies_robot():\n",
    "        P_pi, r_pi = build_P_r_for_policy(P, pi)\n",
    "        v = evaluate_policy_linear_system(P_pi, r_pi, gamma)\n",
    "        vH = float(v[0])\n",
    "        \n",
    "        if best_vH is None or vH > best_vH:\n",
    "            best_vH = vH\n",
    "            best_pi = pi\n",
    "    \n",
    "    return best_pi, best_vH\n",
    "\n",
    "\n",
    "# ========== A1.3: Znajdowanie π* ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"A1.3: OPTYMALIZACJA - Znajdowanie najlepszej polityki π*\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "P, nS, nA = build_recycling_robot_P()\n",
    "action_name = {0: \"SEARCH\", 1: \"WAIT\", 2: \"RECHARGE\"}\n",
    "\n",
    "# Znalezienie π*\n",
    "pi_star, vH_star = best_policy_robot(P, gamma=0.9)\n",
    "\n",
    "print(f\"\\nOptymalna polityka π* dla domyślnych parametrów:\")\n",
    "print(f\"  Parametry: α=0.8, β=0.4, r_search=5.0, r_wait=1.0, rescue_cost=-3.0, γ=0.9\")\n",
    "print(f\"\\n  π*(H) = {action_name[int(np.argmax(pi_star[0]))]}\")\n",
    "print(f\"  π*(L) = {action_name[int(np.argmax(pi_star[1]))]}\")\n",
    "print(f\"\\n  Wartość: v_π*(H) = {vH_star:.4f}\")\n",
    "\n",
    "# Obliczenie v* dla obu stanów\n",
    "P_pi_star, r_pi_star = build_P_r_for_policy(P, pi_star)\n",
    "v_star = evaluate_policy_linear_system(P_pi_star, r_pi_star, 0.9)\n",
    "\n",
    "print(f\"  Wartość: v_π*(L) = {v_star[1]:.4f}\")\n",
    "\n",
    "print(\"\\nWNIOSEK:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"• Optymalnie jest szukać (SEARCH) gdy mamy energię (stan H)\")\n",
    "print(\"• Optymalnie jest ładować (RECHARGE) gdy energia się kończy (stan L)\")\n",
    "print(\"• Ta polityka osiąga najlepszy kompromis między zyskiem a bezpieczeństwem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c38d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 6: Analiza Wrażliwości - Sweep Parametrów (A1.4)\n",
    "\n",
    "### KLUCZOWA OBSERWACJA:\n",
    "\n",
    "**Parametry środowiska (α, β, rescue_cost, γ) są CZĘŚCIĄ ŚWIATA, nie polityki.**\n",
    "\n",
    "Ale wpływają na Q-wartości akcji, co zmienia **którą politykę system powinien wybrać**!\n",
    "\n",
    "### Eksperyment:\n",
    "\n",
    "Utrzymując stałe:\n",
    "- `α = 0.8` (SEARCH w H jest bezpieczny)\n",
    "- `r_search = 5.0, r_wait = 1.0`\n",
    "- `γ = 0.9`\n",
    "\n",
    "Zmieniamy:\n",
    "- **`β`** (od 0.1 do 0.9): jak bezpieczne jest SEARCH w stanie L?\n",
    "- **`rescue_cost`** (od -1.0 do -10.0): jak wielka jest kara za rozładowanie?\n",
    "\n",
    "Obserwujemy: **Kiedy zmienia się π*(L)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db5fe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "A1.4: SWEEP PARAMETRÓW - Wrażliwość na zmiany środowiska\n",
      "======================================================================\n",
      "\n",
      "Ustawienia STAŁE:\n",
      "  α = 0.8      (SEARCH w H jest bezpieczny)\n",
      "  r_search = 5.0\n",
      "  r_wait = 1.0\n",
      "  γ = 0.9      (dyskontowanie)\n",
      "\n",
      "Ustawienia ZMIENNE:\n",
      "  β: [0.1, 0.3, 0.5, 0.7, 0.9]      (bezpieczeństwo SEARCH w L)\n",
      "  rescue_cost: [-1.0, -3.0, -6.0, -10.0]  (kara za rozładowanie)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "WYNIKI SWEEPÓW:\n",
      "----------------------------------------------------------------------\n",
      " beta  rescue_cost  π*(H)    π*(L)\n",
      "  0.1         -1.0 SEARCH RECHARGE\n",
      "  0.1         -3.0 SEARCH RECHARGE\n",
      "  0.1         -6.0 SEARCH RECHARGE\n",
      "  0.1        -10.0 SEARCH RECHARGE\n",
      "  0.3         -1.0 SEARCH RECHARGE\n",
      "  0.3         -3.0 SEARCH RECHARGE\n",
      "  0.3         -6.0 SEARCH RECHARGE\n",
      "  0.3        -10.0 SEARCH RECHARGE\n",
      "  0.5         -1.0 SEARCH   SEARCH\n",
      "  0.5         -3.0 SEARCH RECHARGE\n",
      "  0.5         -6.0 SEARCH RECHARGE\n",
      "  0.5        -10.0 SEARCH RECHARGE\n",
      "  0.7         -1.0 SEARCH   SEARCH\n",
      "  0.7         -3.0 SEARCH RECHARGE\n",
      "  0.7         -6.0 SEARCH RECHARGE\n",
      "  0.7        -10.0 SEARCH RECHARGE\n",
      "  0.9         -1.0 SEARCH   SEARCH\n",
      "  0.9         -3.0 SEARCH   SEARCH\n",
      "  0.9         -6.0 SEARCH   SEARCH\n",
      "  0.9        -10.0 SEARCH   SEARCH\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ANALIZA ZMIAN π*(L):\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "β = 0.1: π*(L) zawsze = RECHARGE\n",
      "\n",
      "β = 0.3: π*(L) zawsze = RECHARGE\n",
      "\n",
      "β = 0.5:\n",
      "  rescue_cost =   -1.0 → π*(L) = SEARCH\n",
      "  rescue_cost =   -3.0 → π*(L) = RECHARGE\n",
      "  rescue_cost =   -6.0 → π*(L) = RECHARGE\n",
      "  rescue_cost =  -10.0 → π*(L) = RECHARGE\n",
      "\n",
      "β = 0.7:\n",
      "  rescue_cost =   -1.0 → π*(L) = SEARCH\n",
      "  rescue_cost =   -3.0 → π*(L) = RECHARGE\n",
      "  rescue_cost =   -6.0 → π*(L) = RECHARGE\n",
      "  rescue_cost =  -10.0 → π*(L) = RECHARGE\n",
      "\n",
      "β = 0.9: π*(L) zawsze = SEARCH\n",
      "\n",
      "======================================================================\n",
      "WNIOSKI:\n",
      "======================================================================\n",
      "\n",
      "1. π*(H) ZAWSZE = SEARCH\n",
      "   → Gdy energia wysoka, zawsze opłaca się szukać\n",
      "\n",
      "2. π*(L) ZMIENIA się w zależności od PARAMETRÓW:\n",
      "\n",
      "   a) Przy MAŁYM β (SEARCH w L jest RYZYKOWNY):\n",
      "      → Dominuje RECHARGE (bezpieczne ładowanie)\n",
      "      → Robot wolej ładować niż ryzykować rozładowanie\n",
      "\n",
      "   b) Przy DUŻYM β (SEARCH w L jest BEZPIECZNY):\n",
      "      → Dominuje SEARCH (zbieranie puszek)\n",
      "      → Robot wie że się nie rozładuje, więc szuka\n",
      "\n",
      "   c) Przy MAŁYM rescue_cost (kara -1.0):\n",
      "      → Większa skłonność do SEARCH w L\n",
      "      → Mała kara = można ryzykować\n",
      "\n",
      "   d) Przy DUŻYM rescue_cost (kara -10.0):\n",
      "      → Większa skłonność do RECHARGE w L\n",
      "      → Duża kara = lepiej być bezpiecznym\n",
      "\n",
      "3. PRÓG DECYZYJNY:\n",
      "   → Istnieje punkt przejścia (dla β ≈ 0.5-0.7)\n",
      "   → Gdzie π*(L) zmienia się z RECHARGE na SEARCH\n",
      "   → Dokładna wartość zależy od rescue_cost\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_sweep(beta_list=None, rescue_list=None, alpha=0.8, r_search=5.0, r_wait=1.0, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Przeprowadza sweep parametrów środowiska i wyznacza π* dla każdej kombinacji.\n",
    "    \"\"\"\n",
    "    if beta_list is None:\n",
    "        beta_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    if rescue_list is None:\n",
    "        rescue_list = [-1.0, -3.0, -6.0, -10.0]\n",
    "    \n",
    "    action_name = {0: \"SEARCH\", 1: \"WAIT\", 2: \"RECHARGE\"}\n",
    "    results = []\n",
    "    \n",
    "    for beta in beta_list:\n",
    "        for rescue_cost in rescue_list:\n",
    "            P, _, _ = build_recycling_robot_P(\n",
    "                alpha=alpha, beta=beta,\n",
    "                r_search=r_search, r_wait=r_wait,\n",
    "                rescue_cost=rescue_cost\n",
    "            )\n",
    "            pi_star, _ = best_policy_robot(P, gamma=gamma)\n",
    "            aH = action_name[int(np.argmax(pi_star[0]))]\n",
    "            aL = action_name[int(np.argmax(pi_star[1]))]\n",
    "            results.append({\n",
    "                'beta': beta,\n",
    "                'rescue_cost': rescue_cost,\n",
    "                'π*(H)': aH,\n",
    "                'π*(L)': aL\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ========== A1.4: SWEEP PARAMETRÓW ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"A1.4: SWEEP PARAMETRÓW - Wrażliwość na zmiany środowiska\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nUstawienia STAŁE:\")\n",
    "print(\"  α = 0.8      (SEARCH w H jest bezpieczny)\")\n",
    "print(\"  r_search = 5.0\")\n",
    "print(\"  r_wait = 1.0\")\n",
    "print(\"  γ = 0.9      (dyskontowanie)\")\n",
    "\n",
    "print(\"\\nUstawienia ZMIENNE:\")\n",
    "print(\"  β: [0.1, 0.3, 0.5, 0.7, 0.9]      (bezpieczeństwo SEARCH w L)\")\n",
    "print(\"  rescue_cost: [-1.0, -3.0, -6.0, -10.0]  (kara za rozładowanie)\")\n",
    "\n",
    "# Przeprowadzenie sweepów\n",
    "df_sweep = run_sweep()\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"WYNIKI SWEEPÓW:\")\n",
    "print(\"-\" * 70)\n",
    "print(df_sweep.to_string(index=False))\n",
    "\n",
    "# Analiza: Kiedy zmienia się π*(L)?\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ANALIZA ZMIAN π*(L):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "transitions = []\n",
    "for beta in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    subset = df_sweep[df_sweep['beta'] == beta]\n",
    "    \n",
    "    # Jak się zmienia π*(L) wraz ze wzrostem kary?\n",
    "    actions_for_beta = subset['π*(L)'].unique()\n",
    "    \n",
    "    if len(actions_for_beta) > 1:\n",
    "        print(f\"\\nβ = {beta}:\")\n",
    "        for rescue in subset['rescue_cost'].unique():\n",
    "            action = subset[subset['rescue_cost'] == rescue]['π*(L)'].values[0]\n",
    "            print(f\"  rescue_cost = {rescue:6.1f} → π*(L) = {action}\")\n",
    "    else:\n",
    "        action = actions_for_beta[0]\n",
    "        print(f\"\\nβ = {beta}: π*(L) zawsze = {action}\")\n",
    "\n",
    "# Wniosek\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WNIOSKI:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. π*(H) ZAWSZE = SEARCH\n",
    "   → Gdy energia wysoka, zawsze opłaca się szukać\n",
    "   \n",
    "2. π*(L) ZMIENIA się w zależności od PARAMETRÓW:\n",
    "   \n",
    "   a) Przy MAŁYM β (SEARCH w L jest RYZYKOWNY):\n",
    "      → Dominuje RECHARGE (bezpieczne ładowanie)\n",
    "      → Robot wolej ładować niż ryzykować rozładowanie\n",
    "   \n",
    "   b) Przy DUŻYM β (SEARCH w L jest BEZPIECZNY):\n",
    "      → Dominuje SEARCH (zbieranie puszek)\n",
    "      → Robot wie że się nie rozładuje, więc szuka\n",
    "   \n",
    "   c) Przy MAŁYM rescue_cost (kara -1.0):\n",
    "      → Większa skłonność do SEARCH w L\n",
    "      → Mała kara = można ryzykować\n",
    "   \n",
    "   d) Przy DUŻYM rescue_cost (kara -10.0):\n",
    "      → Większa skłonność do RECHARGE w L\n",
    "      → Duża kara = lepiej być bezpiecznym\n",
    "\n",
    "3. PRÓG DECYZYJNY:\n",
    "   → Istnieje punkt przejścia (dla β ≈ 0.5-0.7)\n",
    "   → Gdzie π*(L) zmienia się z RECHARGE na SEARCH\n",
    "   → Dokładna wartość zależy od rescue_cost\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c95c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 7: Dodatkowe Eksperymenty - Wpływ Gamma na π*\n",
    "\n",
    "### Pytanie:\n",
    "Jak współczynnik dyskontowania (γ) wpływa na optymalną politykę?\n",
    "\n",
    "**Intuicja:**\n",
    "- γ niskie (0.5): Agent myśli krótkoterminowo → preferuje szybkie nagrody\n",
    "- γ wysokie (0.99): Agent myśli długoterminowo → bardziej ostrożny, planuje na przyszłość"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83315093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EKSPERYMENT: Wpływ Gamma (dyskontowania) na π*\n",
      "======================================================================\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "GAMMA = 0.50\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  β rescue  π*(H)    π*(L) v(H)\n",
      "0.3     -3 SEARCH RECHARGE 9.09\n",
      "0.3    -10 SEARCH RECHARGE 9.09\n",
      "0.7     -3 SEARCH   SEARCH 9.36\n",
      "0.7    -10 SEARCH RECHARGE 9.09\n",
      "0.9     -3 SEARCH   SEARCH 9.75\n",
      "0.9    -10 SEARCH   SEARCH 9.54\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "GAMMA = 0.90\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  β rescue  π*(H)    π*(L)  v(H)\n",
      "0.3     -3 SEARCH RECHARGE 42.37\n",
      "0.3    -10 SEARCH RECHARGE 42.37\n",
      "0.7     -3 SEARCH RECHARGE 42.37\n",
      "0.7    -10 SEARCH RECHARGE 42.37\n",
      "0.9     -3 SEARCH   SEARCH 46.11\n",
      "0.9    -10 SEARCH   SEARCH 42.70\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "GAMMA = 0.99\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  β rescue  π*(H)    π*(L)   v(H)\n",
      "0.3     -3 SEARCH RECHARGE 417.36\n",
      "0.3    -10 SEARCH RECHARGE 417.36\n",
      "0.7     -3 SEARCH RECHARGE 417.36\n",
      "0.7    -10 SEARCH RECHARGE 417.36\n",
      "0.9     -3 SEARCH   SEARCH 448.40\n",
      "0.9    -10 SEARCH RECHARGE 417.36\n",
      "\n",
      "======================================================================\n",
      "OBSERWACJE:\n",
      "======================================================================\n",
      "\n",
      "• γ = 0.50 (myślenie krótkoterminowe):\n",
      "  - Preferuje natychmiastowe nagrody\n",
      "  - Może być bardziej agresywny w L (SEARCH)\n",
      "\n",
      "• γ = 0.90 (balans):\n",
      "  - Równowaga między teraźniejszością a przyszłością\n",
      "  - Standartowy wybór w wielu aplikacjach\n",
      "\n",
      "• γ = 0.99 (myślenie długoterminowe):\n",
      "  - Zdyskontowane nagrody przyszłych kroków są ważne\n",
      "  - Może być bardziej konserwatywny (RECHARGE)\n",
      "  - Agent bardziej planuje na przyszłość\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EKSPERYMENT: Wpływ Gamma (dyskontowania) na π*\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gamma_values = [0.5, 0.9, 0.99]\n",
    "beta_test = [0.3, 0.7, 0.9]\n",
    "rescue_test = [-3.0, -10.0]\n",
    "\n",
    "for gamma_test in gamma_values:\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"GAMMA = {gamma_test:0.2f}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "    \n",
    "    results_gamma = []\n",
    "    \n",
    "    for beta in beta_test:\n",
    "        for rescue_cost in rescue_test:\n",
    "            P, _, _ = build_recycling_robot_P(\n",
    "                alpha=0.8, beta=beta,\n",
    "                r_search=5.0, r_wait=1.0,\n",
    "                rescue_cost=rescue_cost\n",
    "            )\n",
    "            pi_star, vH = best_policy_robot(P, gamma=gamma_test)\n",
    "            \n",
    "            aH = \"SEARCH\" if int(np.argmax(pi_star[0])) == 0 else \"WAIT\"\n",
    "            aL_idx = int(np.argmax(pi_star[1]))\n",
    "            aL = [\"SEARCH\", \"WAIT\", \"RECHARGE\"][aL_idx]\n",
    "            \n",
    "            results_gamma.append({\n",
    "                'β': f\"{beta:.1f}\",\n",
    "                'rescue': f\"{rescue_cost:.0f}\",\n",
    "                'π*(H)': aH,\n",
    "                'π*(L)': aL,\n",
    "                'v(H)': f\"{vH:.2f}\"\n",
    "            })\n",
    "    \n",
    "    df_gamma = pd.DataFrame(results_gamma)\n",
    "    print(df_gamma.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OBSERWACJE:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "• γ = 0.50 (myślenie krótkoterminowe):\n",
    "  - Preferuje natychmiastowe nagrody\n",
    "  - Może być bardziej agresywny w L (SEARCH)\n",
    "  \n",
    "• γ = 0.90 (balans):\n",
    "  - Równowaga między teraźniejszością a przyszłością\n",
    "  - Standartowy wybór w wielu aplikacjach\n",
    "  \n",
    "• γ = 0.99 (myślenie długoterminowe):\n",
    "  - Zdyskontowane nagrody przyszłych kroków są ważne\n",
    "  - Może być bardziej konserwatywny (RECHARGE)\n",
    "  - Agent bardziej planuje na przyszłość\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aedce4",
   "metadata": {},
   "source": [
    "## Sekcja 8: Podsumowanie i Wnioski\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5d4fb",
   "metadata": {},
   "source": [
    "###  STRUKTURA MDP\n",
    "\n",
    "- **Model P[s][a]**: opisuje dynamikę świata (przejścia, nagrody)\n",
    "- **Polityka π[s,a]**: opisuje zachowanie agenta (wybór akcji)  \n",
    "- **Funkcja wartości v_π[s]**: mierzy jakość polityki\n",
    "\n",
    "**Wniosek**: Te trzy komponenty są **niezależne** ale **powiązane**!\n",
    "\n",
    "### EWALUACJA POLITYKI (A1.2)\n",
    "\n",
    "* Dla zadanej polityki możemy obliczyć dokładnie $v_\\pi$  \n",
    "* Rozwiązując układ równań liniowych: $(I - \\gamma P_\\pi)v = r_\\pi$  \n",
    "* Pozwala porównywać różne strategie\n",
    "\n",
    "\n",
    "### OPTYMALIZACJA (A1.3)\n",
    "\n",
    "* Polityka optymalna $\\pi_*$ maksymalizuje wartość oczekiwaną  \n",
    "* Dla małych MDP możemy znaleźć $\\pi_*$ brute force'em  \n",
    "* Dla dużych MDP używamy algorytmów DP (rozdz. 4)\n",
    "\n",
    "\n",
    "### WRAŻLIWOŚĆ NA PARAMETRY (A1.4)\n",
    "\n",
    "* Parametry środowiska (α, β, rescue_cost, γ) wpływają na $\\pi_*$  \n",
    "* Istnieją **progi decyzyjne** - gdzie $\\pi_*$ zmienia się skokowo  \n",
    "* Ta sama struktura problemu może mieć **różne π*** dla różnych środowisk\n",
    "\n",
    "\n",
    "### PRAKTYCZNE IMPLIKACJE\n",
    "\n",
    "* Ważne jest rozumieć **jakie są parametry naszego świata**  \n",
    "* Nie wszystkie strategie są optymalne w każdym środowisku  \n",
    "* **Dobry agent** musi się dostosowywać do zmian otoczenia\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
